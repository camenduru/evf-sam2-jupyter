{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/evf-sam2-jupyter/blob/main/evf_sam2_jupyter.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b dev https://github.com/camenduru/evf-sam2-hf /content/evf-sam2\n",
        "\n",
        "!pip install xformers==0.0.28 transformers==4.31.0 hydra-core timm torchscale gradio SentencePiece accelerate==0.25.0\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/evf-sam2/raw/main/config.json -d /content/evf-sam2/models -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/evf-sam2/resolve/main/model.safetensors -d /content/evf-sam2/models -o model.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/evf-sam2/resolve/main/sentencepiece.bpe.model -d /content/evf-sam2/models -o sentencepiece.bpe.model\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/evf-sam2/raw/main/special_tokens_map.json -d /content/evf-sam2/models -o special_tokens_map.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/evf-sam2/raw/main/tokenizer_config.json -d /content/evf-sam2/models -o tokenizer_config.json\n",
        "\n",
        "%cd /content/evf-sam2\n",
        "\n",
        "import os\n",
        "from inference import sam_preprocess, beit3_preprocess\n",
        "from model.evf_sam2 import EvfSam2Model\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "version = \"/content/evf-sam2/models\"\n",
        "model_type = \"sam2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(version, padding_side=\"right\", use_fast=False)\n",
        "kwargs = {\"torch_dtype\": torch.half}\n",
        "image_model = EvfSam2Model.from_pretrained(version, low_cpu_mem_usage=True, **kwargs)\n",
        "del image_model.visual_model.memory_encoder\n",
        "del image_model.visual_model.memory_attention\n",
        "image_model = image_model.eval()\n",
        "image_model.to('cuda')\n",
        "\n",
        "@torch.no_grad()\n",
        "def inference_image(image_np, prompt):\n",
        "    original_size_list = [image_np.shape[:2]]\n",
        "    image_beit = beit3_preprocess(image_np, 224).to(dtype=image_model.dtype, device=image_model.device)\n",
        "    image_sam, resize_shape = sam_preprocess(image_np, model_type=model_type)\n",
        "    image_sam = image_sam.to(dtype=image_model.dtype, device=image_model.device)\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device=image_model.device)\n",
        "    pred_mask = image_model.inference(\n",
        "        image_sam.unsqueeze(0),\n",
        "        image_beit.unsqueeze(0),\n",
        "        input_ids,\n",
        "        resize_list=[resize_shape],\n",
        "        original_size_list=original_size_list,\n",
        "    )\n",
        "    pred_mask = pred_mask.detach().cpu().numpy()[0]\n",
        "    pred_mask = pred_mask > 0\n",
        "    # visualization = image_np.copy()\n",
        "    # visualization[pred_mask] = (image_np * 0.5 + pred_mask[:, :, None].astype(np.uint8) * np.array([50, 120, 220]) * 0.5)[pred_mask]\n",
        "    visualization = np.dstack([image_np, np.where(pred_mask, 255, 0)])\n",
        "    return visualization / 255.0\n",
        "\n",
        "desc = \"\"\"\n",
        "<div><h2>EVF-SAM-2</h2>\n",
        "<div><h4>EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything Model</h4>\n",
        "<p>EVF-SAM extends <b>SAM-2</>'s capabilities with text-prompted segmentation, achieving high accuracy in Referring Expression Segmentation.</p></div>\n",
        "<div style='display:flex; gap: 0.25rem; align-items: center'><a href=\"https://arxiv.org/abs/2406.20076\"><img src=\"https://img.shields.io/badge/arXiv-Paper-red\"></a><a href=\"https://github.com/hustvl/EVF-SAM\"><img src=\"https://img.shields.io/badge/GitHub-Code-blue\"></a></div>\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(analytics_enabled=False) as demo:\n",
        "    gr.Markdown(desc)\n",
        "    with gr.Row():\n",
        "        input_image = gr.Image(type='numpy', label='Input Image', image_mode='RGB')\n",
        "        output_image = gr.Image(type='numpy', label='Output Image', format='png')\n",
        "    with gr.Row():\n",
        "        image_prompt = gr.Textbox(label=\"Prompt\", info=\"Use a phrase or sentence to describe the object you want to segment. Currently we only support English\")\n",
        "        submit_image = gr.Button(value='Submit', scale=1, variant='primary')\n",
        "    submit_image.click(fn=inference_image, inputs=[input_image, image_prompt], outputs=output_image)\n",
        "demo.launch(share=True, inline=False, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
